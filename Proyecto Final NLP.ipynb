{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cellular-zealand",
   "metadata": {},
   "source": [
    "## Proyecto final procesamiento de lenguaje natural\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "* Laura del Pilar Torres Toro - Código: 617202012\n",
    "* Gabriel Cruz Parra - Código: 617202013\n",
    "\n",
    "**Planteamiento del Problema:**\n",
    "\n",
    "* Determinar la variación de opinión de x personajes políticos en los últimos 10 años respecto a un tema determinado.\n",
    "\n",
    "**Motivación:**\n",
    "\n",
    "* Determinar si en la clase política colombiana existe una tendencia a contradecir sus convicciones.\n",
    "\n",
    "**Metodología:**\n",
    "\n",
    "* Adquisición de los datos \n",
    "* Pre procesamiento (palabras vacías, caracteres especiales, traducción y agregación por año)\n",
    "* Clasificación (análisis de sentimientos)\n",
    "* Determinación de variación\n",
    "* Clustering\n",
    "* Word Cloud\n",
    "* LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dynamic-forth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-03 01:19:22,923 loading file C:\\Users\\Usuario\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import six\n",
    "import ipywidgets as widgets\n",
    "from google.cloud import translate_v2 as translate\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.models import LdaModel\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import PIL.Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "translate_client = translate.Client()\n",
    "stopwords_sp = stopwords.words('spanish')\n",
    "classifier = TextClassifier.load('sentiment')\n",
    "#cambiar el valor de la constante para cargar archivos\n",
    "RUTA_ARCHIVOS = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "billion-metro",
   "metadata": {
    "require": [
     "base/js/events",
     "datatables.net",
     "d3",
     "chartjs",
     "dt-config",
     "dt-components",
     "dt-graph-objects",
     "dt-toolbar",
     "dt-tooltips",
     "jupyter-datatables"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listadoPoliticos = pd.read_excel(f'{RUTA_ARCHIVOS}listadoPoliticos.xlsx')\n",
    "\n",
    "listadoTemas = pd.read_excel(f'{RUTA_ARCHIVOS}listadoPoliticos.xlsx', sheet_name='Temas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "essential-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opcionesPoliticos= []\n",
    "for index, row in listadoPoliticos.iterrows():\n",
    "    opcionesPoliticos.append((row[\"Politico\"]+\" - \"+row[\"Cargo\"], row[\"Cuenta\"]))\n",
    "\n",
    "opcionesPoliticos = sorted(opcionesPoliticos)\n",
    "\n",
    "opcionesTemas = []\n",
    "for index, row in listadoTemas.iterrows():\n",
    "    opcionesTemas.append(row[\"Tema\"])\n",
    "\n",
    "opcionesTemas = sorted(opcionesTemas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impaired-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Función de recuperación de tweets\n",
    "def get_tweet_dataset(maxTweets):\n",
    "    politico = dropdown_politicos.value\n",
    "    tema = dropdown_temas.value\n",
    "    now = datetime.now()\n",
    "    retro_search = now - timedelta(days = (365*text_vigencias.value))\n",
    "    hoy = now.strftime(\"%Y-%m-%d\")\n",
    "    pasado = retro_search.strftime(\"%Y-%m-%d\")\n",
    "    tweet_list = []\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{tema} since:{pasado} until:{hoy} from:{politico}').get_items()) :\n",
    "        if i > maxTweets :\n",
    "            break\n",
    "            \n",
    "        tweet_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.displayname, tweet])\n",
    "    \n",
    "    # Creating a dataframe from the tweets list above \n",
    "    tweets_df = pd.DataFrame(tweet_list, columns=['Fecha', 'tweet_id', 'contenido', 'usuario', 'usuario_mostrar', 'enlace'])\n",
    "    return tweets_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reflected-louis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n",
      "<>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<ipython-input-5-933cf10d9c41>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<ipython-input-5-933cf10d9c41>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definición de función de pre-procesamiento\n",
    "def preProcesado(texto):\n",
    "    pattern1 = '?P<pic>pic.twitter.com/[^\\s]+'\n",
    "    pattern2 = '?P<url>https?://[^\\s]+'\n",
    "    \n",
    "    links = [tuple(j for j in i if j)[-1] for i in re.findall(f\"({pattern1})|({pattern2})\",texto)]\n",
    "    for link in links:\n",
    "        texto = texto.replace(link,\"\")\n",
    "    \n",
    "    hashtags = [interaction for interaction in texto.split() if interaction.startswith(\"#\")]\n",
    "    for hashtag in hashtags:\n",
    "        texto = texto.replace(hashtag, \"\")\n",
    "        \n",
    "    mentions = [interaction for interaction in texto.split() if interaction.startswith(\"@\")]\n",
    "    for mention in mentions:\n",
    "        texto = texto.replace(mention, \"\")\n",
    "    \n",
    "    texto = texto.lower()\n",
    "    #texto = re.sub(r\"[\\W\\d_]+\", \" \", texto)\n",
    "    texto = texto.split() # Tokenizar\n",
    "    #texto = [palabra for palabra in texto if palabra not in stopwords_sp]\n",
    "    #texto = [palabra for palabra in texto if len(palabra)>=2]\n",
    "    texto = \" \".join(texto)\n",
    "    if len(texto.replace(\" \",\"\")) <1:\n",
    "        texto = \"Tweet sin contenido\"\n",
    "        \n",
    "    return texto\n",
    "\n",
    "#función de traducción\n",
    "def traducir(texto, lenguaje):\n",
    "    if isinstance(texto, six.binary_type):\n",
    "        texto = texto.decode(\"utf-8\")\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    resultado = translate_client.translate(texto, target_language=lenguaje)\n",
    "    return resultado['translatedText']\n",
    "\n",
    "# función de aplicación de pre precesado\n",
    "def aplicar_preprocesado(frame):\n",
    "    frame['pre_procesado'] = frame['contenido'].apply(lambda x: preProcesado(x))\n",
    "    frame['pre_procesado'] = frame['pre_procesado'].fillna(\"Tweet sin contenido\")\n",
    "    lenguaje = 'en'\n",
    "    frame['traducido'] = frame['pre_procesado'].apply(lambda x: traducir(x,lenguaje))\n",
    "    frame['Año'] = frame['Fecha'].dt.year\n",
    "    return frame\n",
    "\n",
    "# realización de análisis de sentimientos para cada tweet flair\n",
    "def sent_flair(texto):\n",
    "    sentence = Sentence(texto)\n",
    "    classifier.predict(sentence)\n",
    "    result = sentence.labels[0]\n",
    "    label = result.value\n",
    "    score = result.score\n",
    "    if label == 'POSITIVE':\n",
    "        return score\n",
    "    if label == 'NEGATIVE':\n",
    "        return -1 * score\n",
    "    return score\n",
    "\n",
    "# función aplicación de análisis de sentimientos\n",
    "def aplicar_sentimiento(frame):\n",
    "    frame['textblob_sentiment_en'] = frame['traducido'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    frame['vader_sentiment_en'] = frame['traducido'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
    "    frame['flair_sentiment_en'] = frame['traducido'].apply(lambda x: sent_flair(x))\n",
    "    return frame\n",
    "\n",
    "#Definición de funciones de filtrado\n",
    "def get_positivos(frame, columna, agrupador):\n",
    "    filtro = frame[frame[columna]>0.0]\n",
    "    return filtro.groupby([agrupador]).count()\n",
    "\n",
    "def get_negativos(frame, columna, agrupador):\n",
    "    filtro = frame[frame[columna]<0.0]\n",
    "    return filtro.groupby([agrupador]).count()\n",
    "\n",
    "def get_neutros(frame, columna, agrupador):\n",
    "    filtro = frame[frame[columna]==0.0]\n",
    "    return filtro.groupby([agrupador]).count()\n",
    "\n",
    "\n",
    "# Función de obtención de resumen de tweets por naturaleza\n",
    "def get_resumen(conteoPositivosFlairEn, conteoNegativosFlairEn, conteoNeutrosFlairEn,\n",
    "                        conteoPositivosVaderEn, conteoNegativosVaderEn, conteoNeutrosVaderEn,\n",
    "                        conteoPositivosTextblobEn, conteoNegativosTextblobEn, conteoNeutrosTextblobEn):\n",
    "    conteosData = pd.concat([conteoPositivosFlairEn, conteoNegativosFlairEn, conteoNeutrosFlairEn,\n",
    "                        conteoPositivosVaderEn, conteoNegativosVaderEn, conteoNeutrosVaderEn,\n",
    "                        conteoPositivosTextblobEn, conteoNegativosTextblobEn, conteoNeutrosTextblobEn], axis= 1)\n",
    "\n",
    "    listaValores = ['Año','+ flair(en)', '- flair(en)', '0 flair(en)',\n",
    "               '+ vader(en)', '- vader(en)', '0 vader(en)',\n",
    "               '+ textblob(en)', '- textblob(en)', '0 textblob(en)']\n",
    "    conteosData =pd.DataFrame(conteosData).reset_index()\n",
    "\n",
    "    conteosData.columns = listaValores\n",
    "    conteosData = conteosData.fillna(0)\n",
    "\n",
    "    return conteosData\n",
    "\n",
    "def get_scatter(frame, politico, tema):\n",
    "    fig = px.scatter(frame, y=\"Atributo\", x=\"Año\", color=\"Atributo\", size=\"Cantidad\",\n",
    "                  title=f'Análisis tweets usuario: {politico}; tema: {tema}')\n",
    "    return fig\n",
    "\n",
    "# definir otra forma de obtener resumen\n",
    "def get_resumen_por_año(frame):\n",
    "    dataGraph = frame.T\n",
    "    dataGraph = dataGraph.reset_index()\n",
    "    dataGraph.loc[0:0,\"index\"] = 'Atributo'\n",
    "    headers = dataGraph.iloc[0].astype(str)\n",
    "    dataGraph  = pd.DataFrame(dataGraph.values[1:], columns=headers)\n",
    "    #auxiliar = dataGraph.iloc[0:,1:]\n",
    "    #headers  = auxiliar.columns.to_list()\n",
    "    return dataGraph\n",
    "\n",
    "def get_best_k(x):\n",
    "    sil = []\n",
    "    kmax = 15\n",
    "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "    for k in range(2, kmax+1):\n",
    "      kmeans = KMeans(n_clusters = k).fit(x)\n",
    "      labels = kmeans.labels_\n",
    "      # obtenemos el silhouette score  con sklearn\n",
    "      sil.append(silhouette_score(x, labels, metric = 'euclidean'))\n",
    "    \n",
    "    if round((max(sil)*100), 0) <= 2 or round((max(sil)*100), 0) > 8 :\n",
    "        best = 3\n",
    "    else:\n",
    "        best = int(round((max(sil)*100), 0))\n",
    "    return best  \n",
    "\n",
    "# función para obtener la gráfica de cluster\n",
    "def get_cluster(frame):\n",
    "    \n",
    "    # Matriz TFIDF\n",
    "    tfidf_vect = TfidfVectorizer(preprocessor=preProcesado)\n",
    "    tfidf = tfidf_vect.fit_transform(frame.contenido.values)\n",
    "    matrix = pd.DataFrame(tfidf.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "    \n",
    "    # Obtener el mejor k, si es menor o igual a 2 entonces se dejan 3 grupos\n",
    "    k= get_best_k(matrix)\n",
    "    \n",
    "    # se realiza entrenamiento del modelo con el k obtenido\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(matrix)\n",
    "    # asignando el número de cluster a columna en el frame de datos\n",
    "    frame['cluster'] = model.labels_\n",
    "\n",
    "    # cálculo de vecinos y centroides\n",
    "    nbrs = NearestNeighbors(n_neighbors=3, metric='euclidean').fit(matrix.values)\n",
    "    clust_cnt = frame['cluster'].value_counts()\n",
    "    clust_cnt_pct = frame['cluster'].value_counts(normalize=True)\n",
    "    centroids = model.cluster_centers_\n",
    "    terms = tfidf_vect.get_feature_names()\n",
    "    order_centroids = centroids.argsort()[:, ::-1]\n",
    "    \n",
    "    # Nombrar clusteres en columna del frame de datos\n",
    "    frame['nombres_clusters'] = frame['cluster'].apply(lambda val: \"cluster: \"+str(val))\n",
    "    \n",
    "    # Análisis de componentes principales o reducción de la dimensionalidad\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(matrix)\n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = ['X', 'Y']\n",
    "    result['cluster'] = frame.nombres_clusters.values\n",
    "    result['texto'] = frame.contenido.apply(lambda val: val[:140])\n",
    "    \n",
    "    # Construcción de la gráfica de clústeres\n",
    "    trace = px.scatter(result, y=\"Y\", x=\"X\", color=\"cluster\", hover_name=result['texto'], title=f'Análisis cluster')\n",
    "    return trace\n",
    "    \n",
    "\n",
    "\n",
    "#Modelo de LDA\n",
    "def pre_procesado(texto):  \n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"[\\W\\d]+\", \" \", texto)\n",
    "    texto = texto.split() # tokenización \n",
    "    texto = [palabra for palabra in texto if palabra not in stopwords_sp]\n",
    "    return texto \n",
    "\n",
    "\n",
    "def get_lda(frame):\n",
    "    frame['pre_procesado2'] = frame['pre_procesado'].apply(lambda val: pre_procesado(val))\n",
    "    # Crear una represantación de los documentos en forma de diccionario\n",
    "    dictionary = Dictionary(frame['pre_procesado2'].values)\n",
    "    # Filtrar palabras muy frecuentes o infrecuentes\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.7) \n",
    "    corpus = [dictionary.doc2bow(text) for text in frame['pre_procesado2'].values]\n",
    "    model = LdaModel(corpus = corpus, id2word = dictionary, num_topics = 7, passes=10)\n",
    "\n",
    "    lda_display = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    return lda_display\n",
    "\n",
    "def get_word_cloud(frame, columna):\n",
    "    palabras = frame[columna].str.cat(sep=\" \")\n",
    "    img = PIL.Image.open(f'{RUTA_ARCHIVOS}Mapa13.jpg')\n",
    "    mask = np.array(img)\n",
    "    wordcloud = WordCloud(width=800, height=800,\n",
    "                      max_font_size=150, max_words=200,\n",
    "                      background_color=\"white\", colormap=\"ocean_r\",\n",
    "                      stopwords=stopwords_sp, \n",
    "                      collocations=True,\n",
    "                      mask=mask, contour_width=1, contour_color='blue').generate(palabras) # https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "    return wordcloud\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-thumbnail",
   "metadata": {},
   "source": [
    "### Desarrollo del proyecto\n",
    "\n",
    "* Obtenemos un conjunto de parámetros deseados por el usuario y ejecutamos el análisis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surface-section",
   "metadata": {
    "require": [
     "base/js/events",
     "datatables.net",
     "d3",
     "chartjs",
     "dt-config",
     "dt-components",
     "dt-graph-objects",
     "dt-toolbar",
     "dt-tooltips",
     "jupyter-datatables"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning:\n",
      "\n",
      "`should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "\n",
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\ipywidgets\\widgets\\widget.py:412: DeprecationWarning:\n",
      "\n",
      "Passing unrecognized arguments to super(IntText).__init__(min=100, max=5000).\n",
      "object.__init__() takes exactly one argument (the instance to initialize)\n",
      "This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664b4fdc67114856aa306f046b199b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Político: ', layout=Layout(width='50%'), options=(('Angela Robledo - Repr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3673db4f6bb9441ab799207a82aaca2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=5, description='Años: ', layout=Layout(width='50%'), max=10, min=1), IntText(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2839dfcf9cc4092a74d9dcce09d8912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Procesar', style=ButtonStyle()),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858c80280e314d92843dbe132354ca51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), layout=Layout(h…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir componentes gráficos para los parámetros    \n",
    "dropdown_politicos = widgets.Dropdown(options =opcionesPoliticos, description=\"Político: \", layout=widgets.Layout(width='50%'))\n",
    "dropdown_temas = widgets.Dropdown(options =opcionesTemas, description=\"Temas: \", layout=widgets.Layout(width='50%'))\n",
    "slider_vigencias = widgets.IntSlider(description=\"Años: \", min=1, max=10, step=1, value=5, layout=widgets.Layout(width='50%'))\n",
    "text_vigencias = widgets.IntText(description=\"Años: \")\n",
    "text_max_tweets = widgets.IntText(description=\"Máx tweets: \", value=1000, min=100, max=5000, step=100,\n",
    "                                  layout=widgets.Layout(width='50%'))\n",
    "btn = widgets.Button(description='Procesar')\n",
    "\n",
    "# Sincroniza el valor del slider con un campo de texto\n",
    "widgets.jslink((slider_vigencias, 'value'), (text_vigencias, 'value'))\n",
    "\n",
    "# incluir componentes gráficos en un contenedor horizontal\n",
    "input_widgets = widgets.HBox([dropdown_politicos, dropdown_temas])\n",
    "display(input_widgets)\n",
    "\n",
    "input2_widgets = widgets.HBox([slider_vigencias, text_max_tweets])\n",
    "display(input2_widgets)\n",
    "\n",
    "button_widgets = widgets.HBox([btn])\n",
    "display(button_widgets)\n",
    "\n",
    "# inicializar variables que intervienen en el proceso\n",
    "recuperados = None\n",
    "preprocesados = None\n",
    "sentimientos = None\n",
    "resumen = None\n",
    "conteoPositivosFlairEn = None\n",
    "conteoNegativosFlairEn = None\n",
    "conteoNeutrosFlairEn = None\n",
    "conteoPositivosVaderEn = None\n",
    "conteoNegativosVaderEn = None\n",
    "conteoNeutrosVaderEn = None\n",
    "conteoPositivosTextblobEn = None\n",
    "conteoNegativosTextblobEn = None\n",
    "conteoNeutrosTextblobEn = None\n",
    "\n",
    "# definición de las salidas\n",
    "output_df = widgets.Output()\n",
    "output_pre = widgets.Output()\n",
    "output_sen = widgets.Output()\n",
    "output_res =widgets.Output()\n",
    "plot_output = widgets.Output()\n",
    "cluster_output = widgets.Output()\n",
    "cloud_output = widgets.Output()\n",
    "lda_output = widgets.Output()\n",
    "\n",
    "# Manejador del evento de presionado del botón\n",
    "def btn_eventhandler(obj):\n",
    "    # limpiar las salidas de la pantalla (tabs)\n",
    "    output_df.clear_output()\n",
    "    output_pre.clear_output()\n",
    "    output_sen.clear_output()\n",
    "    output_res.clear_output()\n",
    "    plot_output.clear_output()\n",
    "    cluster_output.clear_output()\n",
    "    lda_output.clear_output()\n",
    "    cloud_output.clear_output()\n",
    "    # Mapeo la lógica de cada salida ( que debe pintar en cada tab)\n",
    "    with output_df:\n",
    "        recuperados = get_tweet_dataset(text_max_tweets.value)\n",
    "        display(recuperados)\n",
    "    with output_pre: \n",
    "        if(len(recuperados)>2):\n",
    "            preprocesados = aplicar_preprocesado(recuperados)\n",
    "            display(preprocesados[[\"Año\",\"contenido\",\"pre_procesado\",\"traducido\"]])\n",
    "        else:   \n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "    with output_sen:\n",
    "        if(len(recuperados)>2):\n",
    "            sentimientos = aplicar_sentimiento(preprocesados)\n",
    "            display(sentimientos[[\"Año\",\"traducido\",\"flair_sentiment_en\",\"vader_sentiment_en\",\"textblob_sentiment_en\"]])\n",
    "        else:\n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "    with output_res:\n",
    "        if (len(recuperados)>2):\n",
    "            # conteos flair inglés\n",
    "            conteoPositivosFlairEn = get_positivos(sentimientos[['flair_sentiment_en','Año']],'flair_sentiment_en','Año')\n",
    "            conteoNegativosFlairEn = get_negativos(sentimientos[['flair_sentiment_en','Año']],'flair_sentiment_en', 'Año')\n",
    "            conteoNeutrosFlairEn = get_neutros(sentimientos[['flair_sentiment_en','Año']],'flair_sentiment_en','Año')\n",
    "\n",
    "            # conteos vader inglés\n",
    "            conteoPositivosVaderEn = get_positivos(sentimientos[['vader_sentiment_en','Año']],'vader_sentiment_en','Año')\n",
    "            conteoNegativosVaderEn = get_negativos(sentimientos[['vader_sentiment_en','Año']],'vader_sentiment_en','Año')\n",
    "            conteoNeutrosVaderEn = get_neutros(sentimientos[['vader_sentiment_en','Año']],'vader_sentiment_en','Año')\n",
    "\n",
    "            # conteos textblob inglés\n",
    "            conteoPositivosTextblobEn = get_positivos(sentimientos[['textblob_sentiment_en','Año']],'textblob_sentiment_en','Año')\n",
    "            conteoNegativosTextblobEn = get_negativos(sentimientos[['textblob_sentiment_en','Año']],'textblob_sentiment_en','Año')\n",
    "            conteoNeutrosTextblobEn = get_neutros(sentimientos[['textblob_sentiment_en','Año']],'textblob_sentiment_en','Año')\n",
    "        \n",
    "            resumen= get_resumen(conteoPositivosFlairEn, conteoNegativosFlairEn, conteoNeutrosFlairEn,\n",
    "                        conteoPositivosVaderEn, conteoNegativosVaderEn, conteoNeutrosVaderEn,\n",
    "                        conteoPositivosTextblobEn, conteoNegativosTextblobEn, conteoNeutrosTextblobEn)\n",
    "            display(resumen)\n",
    "        else:\n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "    with plot_output:\n",
    "        if(len(recuperados)>2):\n",
    "            # conteos flair inglés\n",
    "            conteoPositivosFlairEn = conteoPositivosFlairEn.reset_index()\n",
    "            conteoPositivosFlairEn[\"Atributo\"] = \"+ flair(en)\"\n",
    "            conteoNegativosFlairEn = conteoNegativosFlairEn.reset_index()\n",
    "            conteoNegativosFlairEn[\"Atributo\"] = \"- flair(en)\"\n",
    "            conteoNeutrosFlairEn = conteoNeutrosFlairEn.reset_index()\n",
    "            conteoNeutrosFlairEn[\"Atributo\"] = \"0 flair(en)\"\n",
    "        \n",
    "            flair_df = pd.concat([conteoPositivosFlairEn, conteoNegativosFlairEn, conteoNeutrosFlairEn], axis= 0)\n",
    "            flair_df.columns= ['Año', 'Cantidad', 'Atributo']\n",
    "\n",
    "            # conteos vader inglés\n",
    "            conteoPositivosVaderEn = conteoPositivosVaderEn.reset_index()\n",
    "            conteoPositivosVaderEn[\"Atributo\"] = \"+ vader(en)\"\n",
    "            conteoNegativosVaderEn = conteoNegativosVaderEn.reset_index()\n",
    "            conteoNegativosVaderEn[\"Atributo\"] = \"- vader(en)\"\n",
    "            conteoNeutrosVaderEn = conteoNeutrosVaderEn.reset_index()\n",
    "            conteoNeutrosVaderEn[\"Atributo\"] = \"0 vader(en)\"\n",
    "        \n",
    "            vader_df = pd.concat([conteoPositivosVaderEn, conteoNegativosVaderEn, conteoNeutrosVaderEn], axis= 0)\n",
    "            vader_df.columns= ['Año', 'Cantidad', 'Atributo']\n",
    "\n",
    "            # conteos textblob inglés\n",
    "            conteoPositivosTextblobEn = conteoPositivosTextblobEn.reset_index()\n",
    "            conteoPositivosTextblobEn[\"Atributo\"] = \"+ textblob(en)\"\n",
    "            conteoNegativosTextblobEn = conteoNegativosTextblobEn.reset_index()\n",
    "            conteoNegativosTextblobEn[\"Atributo\"] = \"- textblob(en)\"\n",
    "            conteoNeutrosTextblobEn = conteoNeutrosTextblobEn.reset_index()\n",
    "            conteoNeutrosTextblobEn[\"Atributo\"] = \"0 textblob(en)\"\n",
    "        \n",
    "            textblob_df = pd.concat([conteoPositivosTextblobEn, conteoNegativosTextblobEn, conteoNeutrosTextblobEn], axis= 0)\n",
    "            textblob_df.columns= ['Año', 'Cantidad', 'Atributo']\n",
    "        \n",
    "            c_df = pd.concat([flair_df, vader_df, textblob_df], axis=0)\n",
    "            fig = get_scatter(c_df,dropdown_politicos.value,dropdown_temas.value)\n",
    "            display(fig.show())\n",
    "        else:\n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "    with cluster_output:\n",
    "        if(len(recuperados)>2):\n",
    "            # Obtenemos gráfica del clúster y la enviamos a la salida (con el método display)\n",
    "            fig = get_cluster(preprocesados)\n",
    "            display(fig.show())\n",
    "        else:\n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "    with cloud_output:\n",
    "        # Obtenemos gráfica de word cloud y la enviamos a la salida (con el método display)\n",
    "        if(len(recuperados)>2):\n",
    "            wordcloud = get_word_cloud(preprocesados,\"pre_procesado\")\n",
    "            wordcloudEn =  get_word_cloud(preprocesados,\"traducido\")\n",
    "            \n",
    "            fig = plt.figure(figsize=(10,10))\n",
    "            \n",
    "            ax1 = fig.add_subplot(1,2,1)\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.tight_layout(pad=0)\n",
    "            \n",
    "            ax2 = fig.add_subplot(1,2,2)\n",
    "            plt.imshow(wordcloudEn, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.tight_layout(pad=0)\n",
    "        \n",
    "            display(plt.show())\n",
    "        else:\n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "        \n",
    "    with lda_output:\n",
    "        # obtener gráfica de LDA y enviarla a la salida (con el método display)\n",
    "        if(len(recuperados)>2):\n",
    "            lda_display = get_lda(preprocesados)\n",
    "            display(pyLDAvis.display(lda_display))\n",
    "        else:\n",
    "            display(print(\"Lo sentimos no hemos encontrado suficientes tweets para ejecutar el análisis\"))\n",
    "\n",
    "# asociar al botón el manejador definido        \n",
    "btn.on_click(btn_eventhandler)\n",
    "\n",
    "#Definir los tabs asociando cada una de las salidas \n",
    "tab = widgets.Tab([output_df, output_pre, output_sen, output_res, plot_output, cluster_output, cloud_output, lda_output], \n",
    "                  layout=widgets.Layout(width='100%', height='100%'))\n",
    "tab.set_title(0, 'Conjunto')\n",
    "tab.set_title(1, 'Pre - procesado')\n",
    "tab.set_title(2, 'Sentimientos')\n",
    "tab.set_title(3, 'Resumen')\n",
    "tab.set_title(4, 'Conclusión gráfica')\n",
    "tab.set_title(5, 'Clustering')\n",
    "tab.set_title(6, 'Word Cloud')\n",
    "tab.set_title(7, 'LDA')\n",
    "#mostrar tabs\n",
    "display(tab)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-alaska",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "require": {
   "paths": {
    "buttons.colvis": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.colVis.min",
    "buttons.flash": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.flash.min",
    "buttons.html5": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.html5.min",
    "buttons.print": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.print.min",
    "chartjs": "https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.8.0/Chart",
    "d3": "https://d3js.org/d3.v5.min",
    "d3-array": "https://d3js.org/d3-array.v2.min",
    "datatables.net": "https://cdn.datatables.net/1.10.18/js/jquery.dataTables",
    "datatables.net-buttons": "https://cdn.datatables.net/buttons/1.5.6/js/dataTables.buttons.min",
    "datatables.responsive": "https://cdn.datatables.net/responsive/2.2.2/js/dataTables.responsive.min",
    "datatables.scroller": "https://cdn.datatables.net/scroller/2.0.0/js/dataTables.scroller.min",
    "datatables.select": "https://cdn.datatables.net/select/1.3.0/js/dataTables.select.min",
    "jszip": "https://cdnjs.cloudflare.com/ajax/libs/jszip/2.5.0/jszip.min",
    "moment": "https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.8.0/moment",
    "pdfmake": "https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.36/pdfmake.min",
    "vfsfonts": "https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.36/vfs_fonts"
   },
   "shim": {
    "buttons.colvis": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.flash": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.html5": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.print": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "chartjs": {
     "deps": [
      "moment"
     ]
    },
    "datatables.net": {
     "exports": "$.fn.dataTable"
    },
    "datatables.net-buttons": {
     "deps": [
      "datatables.net"
     ]
    },
    "pdfmake": {
     "deps": [
      "datatables.net"
     ]
    },
    "vfsfonts": {
     "deps": [
      "datatables.net"
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
